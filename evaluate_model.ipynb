{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1eae99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "from seqeval.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    classification_report, accuracy_score\n",
    ")\n",
    "\n",
    "from inference import BusNERInference\n",
    "from constants.entity_labels import ENTITY_LABELS\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb0439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total samples: 200000\n",
      "ðŸ§ª Test samples (10%): 20000\n",
      "âœ… Using same split as training (seed=42)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration (CORRECTED)\n",
    "from datasets import Dataset\n",
    "\n",
    "MODEL_PATH = \"models/bus_ner_transformer_v5\"\n",
    "USE_ONNX = False\n",
    "TEST_DATA_PATH = \"data/training_data_bio.json\"\n",
    "TEST_SPLIT = 0.1  # Same as training\n",
    "SEED = 42  # Same seed as training\n",
    "\n",
    "# Load all data\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# Use SAME split as training (important!)\n",
    "# Convert to HF Dataset format\n",
    "dataset_dict = {\n",
    "    \"tokens\": [sample[\"tokens\"] for sample in all_data],\n",
    "    \"ner_tags\": [sample[\"ner_tags\"] for sample in all_data],\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "split = dataset.train_test_split(test_size=TEST_SPLIT, seed=SEED)\n",
    "\n",
    "# Convert back to our format for evaluation\n",
    "test_indices = split['test']['__index__'] if hasattr(split['test'], '__index__') else None\n",
    "\n",
    "# Get test samples using indices\n",
    "if test_indices:\n",
    "    test_data = [all_data[i] for i in test_indices]\n",
    "else:\n",
    "    # Fallback: use the split directly\n",
    "    test_data = [all_data[i] for i in range(len(split['test']))]\n",
    "\n",
    "print(f\"ðŸ“Š Total samples: {len(all_data)}\")\n",
    "print(f\"ðŸ§ª Test samples (10%): {len(test_data)}\")\n",
    "print(f\"âœ… Using same split as training (seed=42)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04f89f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Helper Functions\n",
    "def extract_entities_from_spans(text: str, entities: List[List[int]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Convert span-based entities to label-based dictionary.\"\"\"\n",
    "    result = {label: [] for label in ENTITY_LABELS}\n",
    "    \n",
    "    for start, end, label in entities:\n",
    "        entity_text = text[start:end].strip()\n",
    "        if entity_text and label in result:\n",
    "            result[label].append(entity_text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def convert_to_bio_format(text: str, entities: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"Convert entity dictionary to BIO tag sequence.\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    \n",
    "    # Build character-to-word mapping\n",
    "    char_to_word = {}\n",
    "    char_pos = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        for i in range(len(word)):\n",
    "            char_to_word[char_pos + i] = word_idx\n",
    "        char_pos += len(word) + 1\n",
    "    \n",
    "    # Mark entities\n",
    "    for label, values in entities.items():\n",
    "        for value in values:\n",
    "            start_idx = text.find(value)\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(value)\n",
    "                word_indices = set()\n",
    "                for char_idx in range(start_idx, end_idx):\n",
    "                    if char_idx in char_to_word:\n",
    "                        word_indices.add(char_to_word[char_idx])\n",
    "                \n",
    "                if word_indices:\n",
    "                    sorted_indices = sorted(word_indices)\n",
    "                    for i, word_idx in enumerate(sorted_indices):\n",
    "                        if i == 0:\n",
    "                            tags[word_idx] = f\"B-{label}\"\n",
    "                        else:\n",
    "                            tags[word_idx] = f\"I-{label}\"\n",
    "    \n",
    "    return tags\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b3c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading model...\n",
      "Loaded PyTorch model from: models/bus_ner_transformer_v5\n",
      "Device: cpu\n",
      "Number of labels: 45\n",
      "âœ… Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model\n",
    "print(\"ðŸ”„ Loading model...\")\n",
    "ner = BusNERInference(MODEL_PATH, use_onnx=USE_ONNX)\n",
    "print(\"âœ… Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa447b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running evaluation...\n",
      "============================================================\n",
      "  Processed 50/20000 samples...\n",
      "  Processed 100/20000 samples...\n",
      "  Processed 150/20000 samples...\n",
      "  Processed 200/20000 samples...\n",
      "  Processed 250/20000 samples...\n",
      "  Processed 300/20000 samples...\n",
      "  Processed 350/20000 samples...\n",
      "  Processed 400/20000 samples...\n",
      "  Processed 450/20000 samples...\n",
      "  Processed 500/20000 samples...\n",
      "  Processed 550/20000 samples...\n",
      "  Processed 600/20000 samples...\n",
      "  Processed 650/20000 samples...\n",
      "  Processed 700/20000 samples...\n",
      "  Processed 750/20000 samples...\n",
      "  Processed 800/20000 samples...\n",
      "  Processed 850/20000 samples...\n",
      "  Processed 900/20000 samples...\n",
      "  Processed 950/20000 samples...\n",
      "  Processed 1000/20000 samples...\n",
      "  Processed 1050/20000 samples...\n",
      "  Processed 1100/20000 samples...\n",
      "  Processed 1150/20000 samples...\n",
      "  Processed 1200/20000 samples...\n",
      "  Processed 1250/20000 samples...\n",
      "  Processed 1300/20000 samples...\n",
      "  Processed 1350/20000 samples...\n",
      "  Processed 1400/20000 samples...\n",
      "  Processed 1450/20000 samples...\n",
      "  Processed 1500/20000 samples...\n",
      "  Processed 1550/20000 samples...\n",
      "  Processed 1600/20000 samples...\n",
      "  Processed 1650/20000 samples...\n",
      "  Processed 1700/20000 samples...\n",
      "  Processed 1750/20000 samples...\n",
      "  Processed 1800/20000 samples...\n",
      "  Processed 1850/20000 samples...\n",
      "  Processed 1900/20000 samples...\n",
      "  Processed 1950/20000 samples...\n",
      "  Processed 2000/20000 samples...\n",
      "  Processed 2050/20000 samples...\n",
      "  Processed 2100/20000 samples...\n",
      "  Processed 2150/20000 samples...\n",
      "  Processed 2200/20000 samples...\n",
      "  Processed 2250/20000 samples...\n",
      "  Processed 2300/20000 samples...\n",
      "  Processed 2350/20000 samples...\n",
      "  Processed 2400/20000 samples...\n",
      "  Processed 2450/20000 samples...\n",
      "  Processed 2500/20000 samples...\n",
      "  Processed 2550/20000 samples...\n",
      "  Processed 2600/20000 samples...\n",
      "  Processed 2650/20000 samples...\n",
      "  Processed 2700/20000 samples...\n",
      "  Processed 2750/20000 samples...\n",
      "  Processed 2800/20000 samples...\n",
      "  Processed 2850/20000 samples...\n",
      "  Processed 2900/20000 samples...\n",
      "  Processed 2950/20000 samples...\n",
      "  Processed 3000/20000 samples...\n",
      "  Processed 3050/20000 samples...\n",
      "  Processed 3100/20000 samples...\n",
      "  Processed 3150/20000 samples...\n",
      "  Processed 3200/20000 samples...\n",
      "  Processed 3250/20000 samples...\n",
      "  Processed 3300/20000 samples...\n",
      "  Processed 3350/20000 samples...\n",
      "  Processed 3400/20000 samples...\n",
      "  Processed 3450/20000 samples...\n",
      "  Processed 3500/20000 samples...\n",
      "  Processed 3550/20000 samples...\n",
      "  Processed 3600/20000 samples...\n",
      "  Processed 3650/20000 samples...\n",
      "  Processed 3700/20000 samples...\n",
      "  Processed 3750/20000 samples...\n",
      "  Processed 3800/20000 samples...\n",
      "  Processed 3850/20000 samples...\n",
      "  Processed 3900/20000 samples...\n",
      "  Processed 3950/20000 samples...\n",
      "  Processed 4000/20000 samples...\n",
      "  Processed 4050/20000 samples...\n",
      "  Processed 4100/20000 samples...\n",
      "  Processed 4150/20000 samples...\n",
      "  Processed 4200/20000 samples...\n",
      "  Processed 4250/20000 samples...\n",
      "  Processed 4300/20000 samples...\n",
      "  Processed 4350/20000 samples...\n",
      "  Processed 4400/20000 samples...\n",
      "  Processed 4450/20000 samples...\n",
      "  Processed 4500/20000 samples...\n",
      "  Processed 4550/20000 samples...\n",
      "  Processed 4600/20000 samples...\n",
      "  Processed 4650/20000 samples...\n",
      "  Processed 4700/20000 samples...\n",
      "  Processed 4750/20000 samples...\n",
      "  Processed 4800/20000 samples...\n",
      "  Processed 4850/20000 samples...\n",
      "  Processed 4900/20000 samples...\n",
      "  Processed 4950/20000 samples...\n",
      "  Processed 5000/20000 samples...\n",
      "  Processed 5050/20000 samples...\n",
      "  Processed 5100/20000 samples...\n",
      "  Processed 5150/20000 samples...\n",
      "  Processed 5200/20000 samples...\n",
      "  Processed 5250/20000 samples...\n",
      "  Processed 5300/20000 samples...\n",
      "  Processed 5350/20000 samples...\n",
      "  Processed 5400/20000 samples...\n",
      "  Processed 5450/20000 samples...\n",
      "  Processed 5500/20000 samples...\n",
      "  Processed 5550/20000 samples...\n",
      "  Processed 5600/20000 samples...\n",
      "  Processed 5650/20000 samples...\n",
      "  Processed 5700/20000 samples...\n",
      "  Processed 5750/20000 samples...\n",
      "  Processed 5800/20000 samples...\n",
      "  Processed 5850/20000 samples...\n",
      "  Processed 5900/20000 samples...\n",
      "  Processed 5950/20000 samples...\n",
      "  Processed 6000/20000 samples...\n",
      "  Processed 6050/20000 samples...\n",
      "  Processed 6100/20000 samples...\n",
      "  Processed 6150/20000 samples...\n",
      "  Processed 6200/20000 samples...\n",
      "  Processed 6250/20000 samples...\n",
      "  Processed 6300/20000 samples...\n",
      "  Processed 6350/20000 samples...\n",
      "  Processed 6400/20000 samples...\n",
      "  Processed 6450/20000 samples...\n",
      "  Processed 6500/20000 samples...\n",
      "  Processed 6550/20000 samples...\n",
      "  Processed 6600/20000 samples...\n",
      "  Processed 6650/20000 samples...\n",
      "  Processed 6700/20000 samples...\n",
      "  Processed 6750/20000 samples...\n",
      "  Processed 6800/20000 samples...\n",
      "  Processed 6850/20000 samples...\n",
      "  Processed 6900/20000 samples...\n",
      "  Processed 6950/20000 samples...\n",
      "  Processed 7000/20000 samples...\n",
      "  Processed 7050/20000 samples...\n",
      "  Processed 7100/20000 samples...\n",
      "  Processed 7150/20000 samples...\n",
      "  Processed 7200/20000 samples...\n",
      "  Processed 7250/20000 samples...\n",
      "  Processed 7300/20000 samples...\n",
      "  Processed 7350/20000 samples...\n",
      "  Processed 7400/20000 samples...\n",
      "  Processed 7450/20000 samples...\n",
      "  Processed 7500/20000 samples...\n",
      "  Processed 7550/20000 samples...\n",
      "  Processed 7600/20000 samples...\n",
      "  Processed 7650/20000 samples...\n",
      "  Processed 7700/20000 samples...\n",
      "  Processed 7750/20000 samples...\n",
      "  Processed 7800/20000 samples...\n",
      "  Processed 7850/20000 samples...\n",
      "  Processed 7900/20000 samples...\n",
      "  Processed 7950/20000 samples...\n",
      "  Processed 8000/20000 samples...\n",
      "  Processed 8050/20000 samples...\n",
      "  Processed 8100/20000 samples...\n",
      "  Processed 8150/20000 samples...\n",
      "  Processed 8200/20000 samples...\n",
      "  Processed 8250/20000 samples...\n",
      "  Processed 8300/20000 samples...\n",
      "  Processed 8350/20000 samples...\n",
      "  Processed 8400/20000 samples...\n",
      "  Processed 8450/20000 samples...\n",
      "  Processed 8500/20000 samples...\n",
      "  Processed 8550/20000 samples...\n",
      "  Processed 8600/20000 samples...\n",
      "  Processed 8650/20000 samples...\n",
      "  Processed 8700/20000 samples...\n",
      "  Processed 8750/20000 samples...\n",
      "  Processed 8800/20000 samples...\n",
      "  Processed 8850/20000 samples...\n",
      "  Processed 8900/20000 samples...\n",
      "  Processed 8950/20000 samples...\n",
      "  Processed 9000/20000 samples...\n",
      "  Processed 9050/20000 samples...\n",
      "  Processed 9100/20000 samples...\n",
      "  Processed 9150/20000 samples...\n",
      "  Processed 9200/20000 samples...\n",
      "  Processed 9250/20000 samples...\n",
      "  Processed 9300/20000 samples...\n",
      "  Processed 9350/20000 samples...\n",
      "  Processed 9400/20000 samples...\n",
      "  Processed 9450/20000 samples...\n",
      "  Processed 9500/20000 samples...\n",
      "  Processed 9550/20000 samples...\n",
      "  Processed 9600/20000 samples...\n",
      "  Processed 9650/20000 samples...\n",
      "  Processed 9700/20000 samples...\n",
      "  Processed 9750/20000 samples...\n",
      "  Processed 9800/20000 samples...\n",
      "  Processed 9850/20000 samples...\n",
      "  Processed 9900/20000 samples...\n",
      "  Processed 9950/20000 samples...\n",
      "  Processed 10000/20000 samples...\n",
      "  Processed 10050/20000 samples...\n",
      "  Processed 10100/20000 samples...\n",
      "  Processed 10150/20000 samples...\n",
      "  Processed 10200/20000 samples...\n",
      "  Processed 10250/20000 samples...\n",
      "  Processed 10300/20000 samples...\n",
      "  Processed 10350/20000 samples...\n",
      "  Processed 10400/20000 samples...\n",
      "  Processed 10450/20000 samples...\n",
      "  Processed 10500/20000 samples...\n",
      "  Processed 10550/20000 samples...\n",
      "  Processed 10600/20000 samples...\n",
      "  Processed 10650/20000 samples...\n",
      "  Processed 10700/20000 samples...\n",
      "  Processed 10750/20000 samples...\n",
      "  Processed 10800/20000 samples...\n",
      "  Processed 10850/20000 samples...\n",
      "  Processed 10900/20000 samples...\n",
      "  Processed 10950/20000 samples...\n",
      "  Processed 11000/20000 samples...\n",
      "  Processed 11050/20000 samples...\n",
      "  Processed 11100/20000 samples...\n",
      "  Processed 11150/20000 samples...\n",
      "  Processed 11200/20000 samples...\n",
      "  Processed 11250/20000 samples...\n",
      "  Processed 11300/20000 samples...\n",
      "  Processed 11350/20000 samples...\n",
      "  Processed 11400/20000 samples...\n",
      "  Processed 11450/20000 samples...\n",
      "  Processed 11500/20000 samples...\n",
      "  Processed 11550/20000 samples...\n",
      "  Processed 11600/20000 samples...\n",
      "  Processed 11650/20000 samples...\n",
      "  Processed 11700/20000 samples...\n",
      "  Processed 11750/20000 samples...\n",
      "  Processed 11800/20000 samples...\n",
      "  Processed 11850/20000 samples...\n",
      "  Processed 11900/20000 samples...\n",
      "  Processed 11950/20000 samples...\n",
      "  Processed 12000/20000 samples...\n",
      "  Processed 12050/20000 samples...\n",
      "  Processed 12100/20000 samples...\n",
      "  Processed 12150/20000 samples...\n",
      "  Processed 12200/20000 samples...\n",
      "  Processed 12250/20000 samples...\n",
      "  Processed 12300/20000 samples...\n",
      "  Processed 12350/20000 samples...\n",
      "  Processed 12400/20000 samples...\n",
      "  Processed 12450/20000 samples...\n",
      "  Processed 12500/20000 samples...\n",
      "  Processed 12550/20000 samples...\n",
      "  Processed 12600/20000 samples...\n",
      "  Processed 12650/20000 samples...\n",
      "  Processed 12700/20000 samples...\n",
      "  Processed 12750/20000 samples...\n",
      "  Processed 12800/20000 samples...\n",
      "  Processed 12850/20000 samples...\n",
      "  Processed 12900/20000 samples...\n",
      "  Processed 12950/20000 samples...\n",
      "  Processed 13000/20000 samples...\n",
      "  Processed 13050/20000 samples...\n",
      "  Processed 13100/20000 samples...\n",
      "  Processed 13150/20000 samples...\n",
      "  Processed 13200/20000 samples...\n",
      "  Processed 13250/20000 samples...\n",
      "  Processed 13300/20000 samples...\n",
      "  Processed 13350/20000 samples...\n",
      "  Processed 13400/20000 samples...\n",
      "  Processed 13450/20000 samples...\n",
      "  Processed 13500/20000 samples...\n",
      "  Processed 13550/20000 samples...\n",
      "  Processed 13600/20000 samples...\n",
      "  Processed 13650/20000 samples...\n",
      "  Processed 13700/20000 samples...\n",
      "  Processed 13750/20000 samples...\n",
      "  Processed 13800/20000 samples...\n",
      "  Processed 13850/20000 samples...\n",
      "  Processed 13900/20000 samples...\n",
      "  Processed 13950/20000 samples...\n",
      "  Processed 14000/20000 samples...\n",
      "  Processed 14050/20000 samples...\n",
      "  Processed 14100/20000 samples...\n",
      "  Processed 14150/20000 samples...\n",
      "  Processed 14200/20000 samples...\n",
      "  Processed 14250/20000 samples...\n",
      "  Processed 14300/20000 samples...\n",
      "  Processed 14350/20000 samples...\n",
      "  Processed 14400/20000 samples...\n",
      "  Processed 14450/20000 samples...\n",
      "  Processed 14500/20000 samples...\n",
      "  Processed 14550/20000 samples...\n",
      "  Processed 14600/20000 samples...\n",
      "  Processed 14650/20000 samples...\n",
      "  Processed 14700/20000 samples...\n",
      "  Processed 14750/20000 samples...\n",
      "  Processed 14800/20000 samples...\n",
      "  Processed 14850/20000 samples...\n",
      "  Processed 14900/20000 samples...\n",
      "  Processed 14950/20000 samples...\n",
      "  Processed 15000/20000 samples...\n",
      "  Processed 15050/20000 samples...\n",
      "  Processed 15100/20000 samples...\n",
      "  Processed 15150/20000 samples...\n",
      "  Processed 15200/20000 samples...\n",
      "  Processed 15250/20000 samples...\n",
      "  Processed 15300/20000 samples...\n",
      "  Processed 15350/20000 samples...\n",
      "  Processed 15400/20000 samples...\n",
      "  Processed 15450/20000 samples...\n",
      "  Processed 15500/20000 samples...\n",
      "  Processed 15550/20000 samples...\n",
      "  Processed 15600/20000 samples...\n",
      "  Processed 15650/20000 samples...\n",
      "  Processed 15700/20000 samples...\n",
      "  Processed 15750/20000 samples...\n",
      "  Processed 15800/20000 samples...\n",
      "  Processed 15850/20000 samples...\n",
      "  Processed 15900/20000 samples...\n",
      "  Processed 15950/20000 samples...\n",
      "  Processed 16000/20000 samples...\n",
      "  Processed 16050/20000 samples...\n",
      "  Processed 16100/20000 samples...\n",
      "  Processed 16150/20000 samples...\n",
      "  Processed 16200/20000 samples...\n",
      "  Processed 16250/20000 samples...\n",
      "  Processed 16300/20000 samples...\n",
      "  Processed 16350/20000 samples...\n",
      "  Processed 16400/20000 samples...\n",
      "  Processed 16450/20000 samples...\n",
      "  Processed 16500/20000 samples...\n",
      "  Processed 16550/20000 samples...\n",
      "  Processed 16600/20000 samples...\n",
      "  Processed 16650/20000 samples...\n",
      "  Processed 16700/20000 samples...\n",
      "  Processed 16750/20000 samples...\n",
      "  Processed 16800/20000 samples...\n",
      "  Processed 16850/20000 samples...\n",
      "  Processed 16900/20000 samples...\n",
      "  Processed 16950/20000 samples...\n",
      "  Processed 17000/20000 samples...\n",
      "  Processed 17050/20000 samples...\n",
      "  Processed 17100/20000 samples...\n",
      "  Processed 17150/20000 samples...\n",
      "  Processed 17200/20000 samples...\n",
      "  Processed 17250/20000 samples...\n",
      "  Processed 17300/20000 samples...\n",
      "  Processed 17350/20000 samples...\n",
      "  Processed 17400/20000 samples...\n",
      "  Processed 17450/20000 samples...\n",
      "  Processed 17500/20000 samples...\n",
      "  Processed 17550/20000 samples...\n",
      "  Processed 17600/20000 samples...\n",
      "  Processed 17650/20000 samples...\n",
      "  Processed 17700/20000 samples...\n",
      "  Processed 17750/20000 samples...\n",
      "  Processed 17800/20000 samples...\n",
      "  Processed 17850/20000 samples...\n",
      "  Processed 17900/20000 samples...\n",
      "  Processed 17950/20000 samples...\n",
      "  Processed 18000/20000 samples...\n",
      "  Processed 18050/20000 samples...\n",
      "  Processed 18100/20000 samples...\n",
      "  Processed 18150/20000 samples...\n",
      "  Processed 18200/20000 samples...\n",
      "  Processed 18250/20000 samples...\n",
      "  Processed 18300/20000 samples...\n",
      "  Processed 18350/20000 samples...\n",
      "  Processed 18400/20000 samples...\n",
      "  Processed 18450/20000 samples...\n",
      "  Processed 18500/20000 samples...\n",
      "  Processed 18550/20000 samples...\n",
      "  Processed 18600/20000 samples...\n",
      "  Processed 18650/20000 samples...\n",
      "  Processed 18700/20000 samples...\n",
      "  Processed 18750/20000 samples...\n",
      "  Processed 18800/20000 samples...\n",
      "  Processed 18850/20000 samples...\n",
      "  Processed 18900/20000 samples...\n",
      "  Processed 18950/20000 samples...\n",
      "  Processed 19000/20000 samples...\n",
      "  Processed 19050/20000 samples...\n",
      "  Processed 19100/20000 samples...\n",
      "  Processed 19150/20000 samples...\n",
      "  Processed 19200/20000 samples...\n",
      "  Processed 19250/20000 samples...\n",
      "  Processed 19300/20000 samples...\n",
      "  Processed 19350/20000 samples...\n",
      "  Processed 19400/20000 samples...\n",
      "  Processed 19450/20000 samples...\n",
      "  Processed 19500/20000 samples...\n",
      "  Processed 19550/20000 samples...\n",
      "  Processed 19600/20000 samples...\n",
      "  Processed 19650/20000 samples...\n",
      "  Processed 19700/20000 samples...\n",
      "  Processed 19750/20000 samples...\n",
      "  Processed 19800/20000 samples...\n",
      "  Processed 19850/20000 samples...\n",
      "  Processed 19900/20000 samples...\n",
      "  Processed 19950/20000 samples...\n",
      "  Processed 20000/20000 samples...\n",
      "âœ… Processed all 20000 samples!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run Evaluation (SIMPLER - uses BIO tags directly)\n",
    "print(\"ðŸ”„ Running evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load label mappings\n",
    "with open('data/id2label.json', 'r') as f:\n",
    "    id2label_dict = json.load(f)\n",
    "    id2label = {int(k): v for k, v in id2label_dict.items()}\n",
    "\n",
    "true_labels_list = []\n",
    "pred_labels_list = []\n",
    "entity_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "for i, sample in enumerate(test_data):\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(test_data)} samples...\")\n",
    "    \n",
    "    # Reconstruct text from tokens\n",
    "    tokens = sample[\"tokens\"]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # Get ground truth BIO tags (convert IDs to labels)\n",
    "    true_ner_tag_ids = sample[\"ner_tags\"]\n",
    "    true_bio = [id2label.get(tag_id, \"O\") for tag_id in true_ner_tag_ids]\n",
    "    \n",
    "    # Get predictions from model\n",
    "    pred_entities = ner.extract(text)\n",
    "    \n",
    "    # Convert predictions to BIO format\n",
    "    # Simple approach: match tokens to predicted entities\n",
    "    pred_bio = [\"O\"] * len(tokens)\n",
    "    \n",
    "    for label, values in pred_entities.items():\n",
    "        for value in values:\n",
    "            value_tokens = value.split()\n",
    "            # Try to find this entity in the token sequence\n",
    "            for j in range(len(tokens) - len(value_tokens) + 1):\n",
    "                if tokens[j:j+len(value_tokens)] == value_tokens:\n",
    "                    # Mark as entity\n",
    "                    for k, token in enumerate(value_tokens):\n",
    "                        if k == 0:\n",
    "                            pred_bio[j + k] = f\"B-{label}\"\n",
    "                        else:\n",
    "                            pred_bio[j + k] = f\"I-{label}\"\n",
    "                    break\n",
    "    \n",
    "    true_labels_list.append(true_bio)\n",
    "    pred_labels_list.append(pred_bio)\n",
    "    \n",
    "    # Extract entities from BIO for per-entity metrics\n",
    "    def extract_from_bio(tokens, bio_tags):\n",
    "        entities = {label: [] for label in ENTITY_LABELS}\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        \n",
    "        for token, tag in zip(tokens, bio_tags):\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if current_label and current_entity:\n",
    "                    entities[current_label].append(\" \".join(current_entity))\n",
    "                current_label = tag[2:]\n",
    "                current_entity = [token]\n",
    "            elif tag.startswith(\"I-\"):\n",
    "                label = tag[2:]\n",
    "                if label == current_label:\n",
    "                    current_entity.append(token)\n",
    "                else:\n",
    "                    if current_label and current_entity:\n",
    "                        entities[current_label].append(\" \".join(current_entity))\n",
    "                    current_label = label\n",
    "                    current_entity = [token]\n",
    "            else:\n",
    "                if current_label and current_entity:\n",
    "                    entities[current_label].append(\" \".join(current_entity))\n",
    "                current_label = None\n",
    "                current_entity = []\n",
    "        \n",
    "        if current_label and current_entity:\n",
    "            entities[current_label].append(\" \".join(current_entity))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    true_entities = extract_from_bio(tokens, true_bio)\n",
    "    pred_entities_dict = extract_from_bio(tokens, pred_bio)\n",
    "    \n",
    "    # Per-entity metrics\n",
    "    for label in ENTITY_LABELS:\n",
    "        true_set = set(true_entities.get(label, []))\n",
    "        pred_set = set(pred_entities_dict.get(label, []))\n",
    "        \n",
    "        entity_metrics[label][\"tp\"] += len(true_set & pred_set)\n",
    "        entity_metrics[label][\"fp\"] += len(pred_set - true_set)\n",
    "        entity_metrics[label][\"fn\"] += len(true_set - pred_set)\n",
    "\n",
    "print(f\"âœ… Processed all {len(test_data)} samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba05dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š OVERALL METRICS\n",
      "============================================================\n",
      "Precision: 0.9788 (97.88%)\n",
      "Recall:    0.9867 (98.67%)\n",
      "F1 Score:  0.9827 (98.27%)\n",
      "Accuracy:  0.9971 (99.71%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Calculate Overall Metrics\n",
    "precision = precision_score(true_labels_list, pred_labels_list)\n",
    "recall = recall_score(true_labels_list, pred_labels_list)\n",
    "f1 = f1_score(true_labels_list, pred_labels_list)\n",
    "accuracy = accuracy_score(true_labels_list, pred_labels_list)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š OVERALL METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387e71bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ PER-ENTITY METRICS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TRAVELER</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AMENITIES</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PRICE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1147</td>\n",
       "      <td>1147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEALS</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEPARTURE_DATE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4850</td>\n",
       "      <td>4850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARRIVAL_DATE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>COUPON_CODE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>537</td>\n",
       "      <td>537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARRIVAL_TIME</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BUS_TYPE</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>2802</td>\n",
       "      <td>2801</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DEPARTURE_TIME</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>1499</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DROP_POINT</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>478</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SEMANTIC</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>1605</td>\n",
       "      <td>1597</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SEAT_TYPE</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>1441</td>\n",
       "      <td>1434</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BUS_FEATURES</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>460</td>\n",
       "      <td>458</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ADD_ONS</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>3930</td>\n",
       "      <td>3891</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OPERATOR</td>\n",
       "      <td>0.9854</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>6399</td>\n",
       "      <td>6330</td>\n",
       "      <td>94</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESTINATION_NAME</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>18413</td>\n",
       "      <td>18153</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOURCE_NAME</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>18298</td>\n",
       "      <td>17969</td>\n",
       "      <td>345</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PICKUP_POINT</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>602</td>\n",
       "      <td>582</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AC_TYPE</td>\n",
       "      <td>0.9626</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>696</td>\n",
       "      <td>669</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOURCE_CITY_CODE</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESTINATION_CITY_CODE</td>\n",
       "      <td>0.1164</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Entity Precision  Recall      F1 Support     TP   FP   FN\n",
       "21               TRAVELER    1.0000  1.0000  1.0000     347    347    0    0\n",
       "13              AMENITIES    1.0000  1.0000  1.0000     349    349    0    0\n",
       "19                  PRICE    1.0000  1.0000  1.0000    1147   1147    0    0\n",
       "17                  DEALS    1.0000  1.0000  1.0000     448    448    0    0\n",
       "4          DEPARTURE_DATE    1.0000  1.0000  1.0000    4850   4850    0    0\n",
       "5            ARRIVAL_DATE    1.0000  1.0000  1.0000      11     11    0    0\n",
       "16            COUPON_CODE    1.0000  1.0000  1.0000     537    537    0    0\n",
       "7            ARRIVAL_TIME    1.0000  1.0000  1.0000     209    209    0    0\n",
       "11               BUS_TYPE    0.9993  0.9996  0.9995    2802   2801    2    1\n",
       "6          DEPARTURE_TIME    0.9993  0.9993  0.9993    1499   1498    1    1\n",
       "9              DROP_POINT    1.0000  0.9958  0.9979     478    476    0    2\n",
       "20               SEMANTIC    0.9975  0.9950  0.9963    1605   1597    4    8\n",
       "12              SEAT_TYPE    0.9965  0.9951  0.9958    1441   1434    5    7\n",
       "14           BUS_FEATURES    0.9913  0.9957  0.9935     460    458    4    2\n",
       "18                ADD_ONS    0.9918  0.9901  0.9910    3930   3891   32   39\n",
       "15               OPERATOR    0.9854  0.9892  0.9873    6399   6330   94   69\n",
       "2        DESTINATION_NAME    0.9859  0.9859  0.9859   18413  18153  260  260\n",
       "0             SOURCE_NAME    0.9812  0.9820  0.9816   18298  17969  345  329\n",
       "8            PICKUP_POINT    0.9966  0.9668  0.9815     602    582    2   20\n",
       "10                AC_TYPE    0.9626  0.9612  0.9619     696    669   26   27\n",
       "1        SOURCE_CITY_CODE    0.1250  1.0000  0.2222      46     46  322    0\n",
       "3   DESTINATION_CITY_CODE    0.1164  1.0000  0.2086      34     34  258    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Numeric version (for analysis):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TRAVELER</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AMENITIES</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PRICE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1147</td>\n",
       "      <td>1147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEALS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEPARTURE_DATE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4850</td>\n",
       "      <td>4850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARRIVAL_DATE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>COUPON_CODE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>537</td>\n",
       "      <td>537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARRIVAL_TIME</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BUS_TYPE</td>\n",
       "      <td>0.999286</td>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>2802</td>\n",
       "      <td>2801</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DEPARTURE_TIME</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>1499</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DROP_POINT</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995816</td>\n",
       "      <td>0.997904</td>\n",
       "      <td>478</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SEMANTIC</td>\n",
       "      <td>0.997502</td>\n",
       "      <td>0.995016</td>\n",
       "      <td>0.996257</td>\n",
       "      <td>1605</td>\n",
       "      <td>1597</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SEAT_TYPE</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>1441</td>\n",
       "      <td>1434</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BUS_FEATURES</td>\n",
       "      <td>0.991342</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.993492</td>\n",
       "      <td>460</td>\n",
       "      <td>458</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ADD_ONS</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.990076</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>3930</td>\n",
       "      <td>3891</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OPERATOR</td>\n",
       "      <td>0.985367</td>\n",
       "      <td>0.989217</td>\n",
       "      <td>0.987288</td>\n",
       "      <td>6399</td>\n",
       "      <td>6330</td>\n",
       "      <td>94</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESTINATION_NAME</td>\n",
       "      <td>0.985880</td>\n",
       "      <td>0.985880</td>\n",
       "      <td>0.985880</td>\n",
       "      <td>18413</td>\n",
       "      <td>18153</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOURCE_NAME</td>\n",
       "      <td>0.981162</td>\n",
       "      <td>0.982020</td>\n",
       "      <td>0.981591</td>\n",
       "      <td>18298</td>\n",
       "      <td>17969</td>\n",
       "      <td>345</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PICKUP_POINT</td>\n",
       "      <td>0.996575</td>\n",
       "      <td>0.966777</td>\n",
       "      <td>0.981450</td>\n",
       "      <td>602</td>\n",
       "      <td>582</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AC_TYPE</td>\n",
       "      <td>0.962590</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>0.961898</td>\n",
       "      <td>696</td>\n",
       "      <td>669</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOURCE_CITY_CODE</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESTINATION_CITY_CODE</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208589</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Entity  Precision    Recall        F1  Support     TP   FP  \\\n",
       "21               TRAVELER   1.000000  1.000000  1.000000      347    347    0   \n",
       "13              AMENITIES   1.000000  1.000000  1.000000      349    349    0   \n",
       "19                  PRICE   1.000000  1.000000  1.000000     1147   1147    0   \n",
       "17                  DEALS   1.000000  1.000000  1.000000      448    448    0   \n",
       "4          DEPARTURE_DATE   1.000000  1.000000  1.000000     4850   4850    0   \n",
       "5            ARRIVAL_DATE   1.000000  1.000000  1.000000       11     11    0   \n",
       "16            COUPON_CODE   1.000000  1.000000  1.000000      537    537    0   \n",
       "7            ARRIVAL_TIME   1.000000  1.000000  1.000000      209    209    0   \n",
       "11               BUS_TYPE   0.999286  0.999643  0.999465     2802   2801    2   \n",
       "6          DEPARTURE_TIME   0.999333  0.999333  0.999333     1499   1498    1   \n",
       "9              DROP_POINT   1.000000  0.995816  0.997904      478    476    0   \n",
       "20               SEMANTIC   0.997502  0.995016  0.996257     1605   1597    4   \n",
       "12              SEAT_TYPE   0.996525  0.995142  0.995833     1441   1434    5   \n",
       "14           BUS_FEATURES   0.991342  0.995652  0.993492      460    458    4   \n",
       "18                ADD_ONS   0.991843  0.990076  0.990959     3930   3891   32   \n",
       "15               OPERATOR   0.985367  0.989217  0.987288     6399   6330   94   \n",
       "2        DESTINATION_NAME   0.985880  0.985880  0.985880    18413  18153  260   \n",
       "0             SOURCE_NAME   0.981162  0.982020  0.981591    18298  17969  345   \n",
       "8            PICKUP_POINT   0.996575  0.966777  0.981450      602    582    2   \n",
       "10                AC_TYPE   0.962590  0.961207  0.961898      696    669   26   \n",
       "1        SOURCE_CITY_CODE   0.125000  1.000000  0.222222       46     46  322   \n",
       "3   DESTINATION_CITY_CODE   0.116438  1.000000  0.208589       34     34  258   \n",
       "\n",
       "     FN  \n",
       "21    0  \n",
       "13    0  \n",
       "19    0  \n",
       "17    0  \n",
       "4     0  \n",
       "5     0  \n",
       "16    0  \n",
       "7     0  \n",
       "11    1  \n",
       "6     1  \n",
       "9     2  \n",
       "20    8  \n",
       "12    7  \n",
       "14    2  \n",
       "18   39  \n",
       "15   69  \n",
       "2   260  \n",
       "0   329  \n",
       "8    20  \n",
       "10   27  \n",
       "1     0  \n",
       "3     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Per-Entity Metrics Table (NO MATPLOTLIB VERSION)\n",
    "per_entity_data = []\n",
    "\n",
    "for label in ENTITY_LABELS:\n",
    "    tp = entity_metrics[label][\"tp\"]\n",
    "    fp = entity_metrics[label][\"fp\"]\n",
    "    fn = entity_metrics[label][\"fn\"]\n",
    "    \n",
    "    precision_entity = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall_entity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_entity = 2 * (precision_entity * recall_entity) / (precision_entity + recall_entity) if (precision_entity + recall_entity) > 0 else 0.0\n",
    "    \n",
    "    per_entity_data.append({\n",
    "        \"Entity\": label,\n",
    "        \"Precision\": precision_entity,\n",
    "        \"Recall\": recall_entity,\n",
    "        \"F1\": f1_entity,\n",
    "        \"Support\": tp + fn,\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(per_entity_data)\n",
    "df = df.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "# Format numbers\n",
    "df_formatted = df.copy()\n",
    "for col in ['Precision', 'Recall', 'F1']:\n",
    "    df_formatted[col] = df_formatted[col].apply(lambda x: f\"{x:.4f}\")\n",
    "for col in ['Support', 'TP', 'FP', 'FN']:\n",
    "    df_formatted[col] = df_formatted[col].apply(lambda x: f\"{int(x)}\")\n",
    "\n",
    "print(\"ðŸ“ˆ PER-ENTITY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "display(df_formatted)\n",
    "\n",
    "# Also show the numeric version for sorting/filtering\n",
    "print(\"\\nðŸ“Š Numeric version (for analysis):\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4cbbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“‹ DETAILED CLASSIFICATION REPORT\n",
      "============================================================\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "              AC_TYPE       0.96      0.96      0.96       698\n",
      "              ADD_ONS       0.99      0.98      0.98      3967\n",
      "            AMENITIES       1.00      1.00      1.00       349\n",
      "         ARRIVAL_DATE       1.00      1.00      1.00        11\n",
      "         ARRIVAL_TIME       1.00      1.00      1.00       210\n",
      "         BUS_FEATURES       0.99      0.99      0.99       462\n",
      "             BUS_TYPE       1.00      1.00      1.00      2803\n",
      "          COUPON_CODE       1.00      1.00      1.00       538\n",
      "                DEALS       1.00      1.00      1.00       449\n",
      "       DEPARTURE_DATE       1.00      1.00      1.00      4850\n",
      "       DEPARTURE_TIME       1.00      1.00      1.00      1505\n",
      "DESTINATION_CITY_CODE       0.12      1.00      0.21        34\n",
      "     DESTINATION_NAME       0.99      0.99      0.99     18413\n",
      "           DROP_POINT       1.00      1.00      1.00       478\n",
      "             OPERATOR       0.99      0.99      0.99      6399\n",
      "         PICKUP_POINT       0.99      0.97      0.98       602\n",
      "                PRICE       1.00      1.00      1.00      1150\n",
      "            SEAT_TYPE       1.00      0.99      0.99      1447\n",
      "             SEMANTIC       1.00      0.99      0.99      1611\n",
      "     SOURCE_CITY_CODE       0.12      1.00      0.22        46\n",
      "          SOURCE_NAME       0.98      0.98      0.98     18298\n",
      "             TRAVELER       1.00      1.00      1.00       348\n",
      "\n",
      "            micro avg       0.98      0.99      0.98     64668\n",
      "            macro avg       0.91      0.99      0.92     64668\n",
      "         weighted avg       0.99      0.99      0.99     64668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Detailed Classification Report\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“‹ DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_labels_list, pred_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7c39e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "[1] Query: I want to know about the Multi axle bus from NCR Delhi to Imphal , specifically from Near Silk Board Bus Stop .\n",
      "   Ground Truth:\n",
      "     SOURCE_NAME: ['NCR Delhi']\n",
      "     DESTINATION_NAME: ['Imphal']\n",
      "     PICKUP_POINT: ['Near Silk Board Bus Stop']\n",
      "     BUS_TYPE: ['Multi axle']\n",
      "   Predicted:\n",
      "     SOURCE_NAME: ['NCR Delhi']\n",
      "     DESTINATION_NAME: ['Imphal']\n",
      "     PICKUP_POINT: ['Near Silk Board Bus Stop']\n",
      "     BUS_TYPE: ['Multi axle']\n",
      "   Comparison:\n",
      "     âœ… DESTINATION_NAME: Correct\n",
      "     âœ… BUS_TYPE: Correct\n",
      "     âœ… PICKUP_POINT: Correct\n",
      "     âœ… SOURCE_NAME: Correct\n",
      "\n",
      "[2] Query: find me the bus options from Kullu to Thiruvananthapuram .\n",
      "   Ground Truth:\n",
      "     SOURCE_NAME: ['Kullu']\n",
      "     DESTINATION_NAME: ['Thiruvananthapuram']\n",
      "   Predicted:\n",
      "     SOURCE_NAME: ['Kullu']\n",
      "     DESTINATION_NAME: ['Thiruvananthapuram']\n",
      "   Comparison:\n",
      "     âœ… DESTINATION_NAME: Correct\n",
      "     âœ… SOURCE_NAME: Correct\n",
      "\n",
      "[3] Query: can you find me bus services from Gachibowli to Imphal ?\n",
      "   Ground Truth:\n",
      "     SOURCE_NAME: ['Gachibowli']\n",
      "     DESTINATION_NAME: ['Imphal']\n",
      "   Predicted:\n",
      "     SOURCE_NAME: ['Gachibowli']\n",
      "     DESTINATION_NAME: ['Imphal']\n",
      "   Comparison:\n",
      "     âœ… DESTINATION_NAME: Correct\n",
      "     âœ… SOURCE_NAME: Correct\n",
      "\n",
      "[4] Query: i want to book both onward and return tickets together with Raveena Travels from Baran to Moradabad . the first option was available earlier , but now it is not showing .\n",
      "   Ground Truth:\n",
      "     SOURCE_NAME: ['Baran']\n",
      "     DESTINATION_NAME: ['Moradabad']\n",
      "     OPERATOR: ['Raveena Travels']\n",
      "   Predicted:\n",
      "     SOURCE_NAME: ['Baran']\n",
      "     DESTINATION_NAME: ['Moradabad']\n",
      "     OPERATOR: ['Raveena Travels']\n",
      "   Comparison:\n",
      "     âœ… DESTINATION_NAME: Correct\n",
      "     âœ… OPERATOR: Correct\n",
      "     âœ… SOURCE_NAME: Correct\n",
      "\n",
      "[5] Query: are your drivers safe , and are your vehicles insured ? i ' m looking for assurance about the safety of drivers and the Cancellationtrip assured status of vehicles for my trip from Ahmedabad to Surat .\n",
      "   Ground Truth:\n",
      "     SOURCE_NAME: ['Ahmedabad']\n",
      "     DESTINATION_NAME: ['Surat']\n",
      "     ADD_ONS: ['Cancellationtrip assured']\n",
      "   Predicted:\n",
      "     SOURCE_NAME: ['Ahmedabad']\n",
      "     DESTINATION_NAME: ['Surat']\n",
      "     ADD_ONS: ['Cancellationtrip assured']\n",
      "   Comparison:\n",
      "     âœ… ADD_ONS: Correct\n",
      "     âœ… DESTINATION_NAME: Correct\n",
      "     âœ… SOURCE_NAME: Correct\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Sample Predictions (Visual Inspection) - CORRECTED\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ” SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show first 5 samples\n",
    "for i, sample in enumerate(test_data[:5]):\n",
    "    # Reconstruct text from tokens\n",
    "    tokens = sample[\"tokens\"]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # Get predictions\n",
    "    pred = ner.extract(text)\n",
    "    \n",
    "    # Get ground truth entities from BIO tags\n",
    "    true_ner_tag_ids = sample[\"ner_tags\"]\n",
    "    true_bio = [id2label.get(tag_id, \"O\") for tag_id in true_ner_tag_ids]\n",
    "    \n",
    "    # Extract ground truth entities\n",
    "    def extract_from_bio(tokens, bio_tags):\n",
    "        entities = {label: [] for label in ENTITY_LABELS}\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        \n",
    "        for token, tag in zip(tokens, bio_tags):\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if current_label and current_entity:\n",
    "                    entities[current_label].append(\" \".join(current_entity))\n",
    "                current_label = tag[2:]\n",
    "                current_entity = [token]\n",
    "            elif tag.startswith(\"I-\"):\n",
    "                label = tag[2:]\n",
    "                if label == current_label:\n",
    "                    current_entity.append(token)\n",
    "                else:\n",
    "                    if current_label and current_entity:\n",
    "                        entities[current_label].append(\" \".join(current_entity))\n",
    "                    current_label = label\n",
    "                    current_entity = [token]\n",
    "            else:\n",
    "                if current_label and current_entity:\n",
    "                    entities[current_label].append(\" \".join(current_entity))\n",
    "                current_label = None\n",
    "                current_entity = []\n",
    "        \n",
    "        if current_label and current_entity:\n",
    "            entities[current_label].append(\" \".join(current_entity))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    true_entities = extract_from_bio(tokens, true_bio)\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Query: {text}\")\n",
    "    print(\"   Ground Truth:\")\n",
    "    for label, values in true_entities.items():\n",
    "        if values:\n",
    "            print(f\"     {label}: {values}\")\n",
    "    \n",
    "    print(\"   Predicted:\")\n",
    "    for label, values in pred.items():\n",
    "        if values:\n",
    "            print(f\"     {label}: {values}\")\n",
    "    \n",
    "    # Show differences\n",
    "    print(\"   Comparison:\")\n",
    "    all_labels = set(list(true_entities.keys()) + list(pred.keys()))\n",
    "    for label in all_labels:\n",
    "        true_vals = set(true_entities.get(label, []))\n",
    "        pred_vals = set(pred.get(label, []))\n",
    "        if true_vals != pred_vals:\n",
    "            missing = true_vals - pred_vals\n",
    "            extra = pred_vals - true_vals\n",
    "            if missing:\n",
    "                print(f\"     âŒ Missing {label}: {list(missing)}\")\n",
    "            if extra:\n",
    "                print(f\"     âš ï¸  Extra {label}: {list(extra)}\")\n",
    "        else:\n",
    "            if true_vals:\n",
    "                print(f\"     âœ… {label}: Correct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
